import time
import sys
import json
import matplotlib.pyplot as plt

start_test_id = 0
end_test_id = 10
max_tokens = 2048
max_input_length = 2048
model_path = [  "/data/leili/models/llama2-7B/",
                "/data/leili/models/opt1.3B",
                "/data/leili/models/opt13B"]
json_file_path = ['/data/leili/datasets/ShareGPT52K/sg_90k_part1.json', 
                  '/data/leili/datasets/ShareGPT52K/sg_90k_part2.json']
save_file_path = ["sharegpt-llama2-7B.txt",
    "sharegpt-opt1_3b.txt", "sharegpt-opt13b.txt"]
save_png_path = ["sharegpt-llama2-7B.png",
    "sharegpt-opt1_3b.png", "sharegpt-opt13b.png"]
model_num = int(sys.argv[1])
prompts = []
token_length = []


# get the prompts generated by human.
for path in json_file_path:
    with open(path, 'r', encoding='utf-8') as file:
        data = json.load(file)

    for entry in data: 
        for topic in entry['conversations']:
            if topic['from'] == 'human':
                if len(topic['value']) < max_input_length \
                    and len(topic['value']) != 0:
                    prompts.append(topic['value'])

print(f"number of requests produced by human: {len(prompts)}\n\
max prompts: {max(len(prompt) for prompt in prompts)}\n\
min prompts: {min(len(prompt) for prompt in prompts)}\n\
avg prompts: {sum(len(prompt) for prompt in prompts)/len(prompts)}")


from vllm import LLM, SamplingParams
# get the output of the prompt using given model.
sampling_params = SamplingParams(temperature=0.8, 
                top_p=0.95, max_tokens=max_tokens)
llm = LLM(model=model_path[model_num])
start_time = time.time()
outputs = llm.generate(prompts[start_test_id:end_test_id], \
                     sampling_params)
end_time = time.time()
print("execution time: ", end_time - start_time)
for output in outputs:
    prompt = output.prompt
    generated_token = output.outputs[0].token_ids
    token_length.append(len(generated_token))
print(f"num of output: {len(token_length)}, max_len: {max(token_length)}\n\
min_len: {min(token_length)}, avg_len: {sum(token_length)/len(token_length)}")
# write the output list into a file in case of wrong graphing.

with open(save_file_path[model_num], 'w') as file:
    for idx in range(len(token_length)):
        number = token_length[idx]
        file.write(f"{len(prompts[idx])} {number}\n")



# plot the graph of the output length.
plt.hist(token_length, bins=max(token_length)-min(token_length)+1,\
            align='left', edgecolor='orange')

plt.title('Output Length Distribution')
plt.xlabel('Prompt')
plt.ylabel('Frequency')

plt.savefig(save_png_path[model_num])

